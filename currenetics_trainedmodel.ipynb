{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPcdUszgR6dH61vmJl3FLHg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ayobamijeje/curenetics_test/blob/model_training_branch/currenetics_trainedmodel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Curenetic Sentiment Analysis\n",
        "\n",
        "\n",
        "#### Load data\n",
        "#### Data cleaning and visualisation\n",
        "#### Generate data for data augmentationn using transformers and nltk\n",
        "#### Model pipeline for ML text transformation\n",
        "#### Train Modoel - DL and Logistic regression\n",
        "#### Save best model  "
      ],
      "metadata": {
        "id": "LYW60GBjzk0m"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DKq0LOFuMOI8"
      },
      "outputs": [],
      "source": [
        "## Libraries\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from wordcloud import WordCloud\n",
        "from collections import Counter\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from sklearn.metrics import classification_report,accuracy_score\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data  = pd.read_csv('/content/feedback_data.csv')\n"
      ],
      "metadata": {
        "id": "RyxA3fODMR6Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head(10)"
      ],
      "metadata": {
        "id": "9gvgXKttMfZw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2 columns, 51 row, string data, no null\n",
        "\n",
        "data.info()\n"
      ],
      "metadata": {
        "id": "Uaufl46IMjKh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Neg 27, Pos 24\n",
        "data['Sentiment'].value_counts().plot(kind = 'barh').invert_yaxis()\n",
        "plt.title('Sentiment review')"
      ],
      "metadata": {
        "id": "gLl3wxd4Mm5-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# No of words - 520\n",
        "lst_text = data['Feedback'].str.cat(sep=',').split(' ')\n",
        "print(len(lst_text))\n"
      ],
      "metadata": {
        "id": "GBsHmqlhMrbf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# stopwords are encompase majority of the text\n",
        "pd.Series(lst_text).value_counts()[:10].plot(kind = 'bar')\n",
        "plt.title('First 10 words')"
      ],
      "metadata": {
        "id": "urirMqr7MuYS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Text Augmentation\n"
      ],
      "metadata": {
        "id": "NmVAgGeM2jCv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import random\n",
        "from nltk.corpus import wordnet\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "# Function to get synonyms\n",
        "def get_synonyms(word):\n",
        "    synonyms = set()\n",
        "    for syn in wordnet.synsets(word):\n",
        "        for lemma in syn.lemmas():\n",
        "            name = lemma.name().replace(\"_\", \" \").lower()\n",
        "            if name != word:\n",
        "                synonyms.add(name)\n",
        "    return list(synonyms)\n",
        "\n",
        "# Basic synonym replacement\n",
        "def augment_sentence(sentence, n=1):\n",
        "    words = sentence.split()\n",
        "    new_sentences = []\n",
        "    for _ in range(n):\n",
        "        new_words = words.copy()\n",
        "        for i, word in enumerate(new_words):\n",
        "            syns = get_synonyms(word)\n",
        "            if syns:\n",
        "                new_words[i] = random.choice(syns)\n",
        "        new_sentences.append(' '.join(new_words))\n",
        "    return ' '.join(new_sentences)\n"
      ],
      "metadata": {
        "id": "RPpHr8OsMx4L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "exp1 = data['Feedback'][:10].apply(lambda x : augment_sentence(x))\n",
        "print(exp1)\n",
        "print('------------------')\n",
        "print(data['Feedback'][:10])"
      ],
      "metadata": {
        "id": "DaWeL0SUM2dx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "paraphraser = pipeline(\n",
        "    \"text2text-generation\",\n",
        "    model=\"Vamsi/T5_Paraphrase_Paws\",\n",
        "    tokenizer=\"Vamsi/T5_Paraphrase_Paws\"\n",
        ")\n",
        "\n",
        "def augment_sentence02(text, num_return_sequences=1):\n",
        "    prompt = f\"paraphrase: {text} </s>\"\n",
        "    results = paraphraser(\n",
        "        prompt,\n",
        "        max_length=30,\n",
        "        num_return_sequences=num_return_sequences,\n",
        "        do_sample=True,\n",
        "        top_k=100,\n",
        "        top_p=0.90\n",
        "    )\n",
        "    return ' '.join([r['generated_text'] for r in results])\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "2OVbdcrzM6vL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "exp2 = data['Feedback'][:10].apply(lambda x : augment_sentence02(x))\n",
        "print(exp2)\n",
        "print('------------------')\n",
        "print(data['Feedback'][:10])\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "UsrRinV5M-dc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['aug_sentence1'] = data['Feedback'].apply(lambda x : (augment_sentence(x)))\n",
        "\n",
        "data['aug_sentence2'] = data['Feedback'].apply(lambda x : (augment_sentence02(x)))\n",
        "\n",
        "\n",
        "data\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "mqhUYFGON3cs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data01 = data[['aug_sentence1', 'Sentiment']]\n",
        "data02 = data[['aug_sentence2', 'Sentiment']]\n",
        "\n",
        "data01.rename(columns = {'aug_sentence1':'Feedback'}, inplace = True)\n",
        "data02.rename(columns = {'aug_sentence2':'Feedback'}, inplace = True)\n",
        "\n",
        "\n",
        "\n",
        "combine_data = pd.concat([data[['Feedback', 'Sentiment']], data01[['Feedback', 'Sentiment']]], ignore_index = True, axis = 0)\n",
        "combine_data = pd.concat([combine_data[['Feedback', 'Sentiment']], data02[['Feedback', 'Sentiment']]], ignore_index = True, axis = 0)\n",
        "\n"
      ],
      "metadata": {
        "id": "zgg29iyvNGhY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "combine_data\n",
        "combine_data.info()"
      ],
      "metadata": {
        "id": "zWxPs2aJOz33"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "\n",
        "stopwords = stopwords.words('english')"
      ],
      "metadata": {
        "id": "C6Nwsl_9QisO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Creating a function to remove stopwords\n",
        "def remove_stopwords(text):\n",
        "    s = []\n",
        "    text = text.lower()\n",
        "    for words in text.split():\n",
        "        if words not in stopwords:\n",
        "            s.append(words)\n",
        "    a = s[:]\n",
        "    s.clear()\n",
        "    return ' '.join(a)"
      ],
      "metadata": {
        "id": "ECi_3G56O5sa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "combine_data['FB_WSW'] = combine_data['Feedback'].apply(lambda x : remove_stopwords(x))\n",
        "\n",
        "combine_data"
      ],
      "metadata": {
        "collapsed": true,
        "id": "024SiyetPHu4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "combine_data['FB_WSW'].apply(lambda x : len(x)).max()"
      ],
      "metadata": {
        "id": "zGQzqBbwQsIz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Hint about words\n",
        "\n",
        "\n",
        "plt.figure(figsize=(10, 30))\n",
        "\n",
        "for dig, i in enumerate(combine_data['Sentiment'].unique()):\n",
        "  text_data = \" \".join(statement for statement in combine_data.loc[combine_data['Sentiment']== i, 'FB_WSW'])\n",
        "  wordcloud = WordCloud(width=800, height=400, background_color=\"white\").generate(text_data)\n",
        "  axes = plt.subplot(4, 2, dig + 1)\n",
        "  plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "  plt.axis(\"off\")\n",
        "  plt.title(i)\n",
        "\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.subplots_adjust(hspace = -0.8)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6_Sk7Ko8QzAS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML text pipiline - text_vectorization\n",
        "### tf text_vectorization  create a pipleline to hand texts helps to raw_text → standardized → tokenized → mapped to if_idf → padded\n"
      ],
      "metadata": {
        "id": "BBraOopb3EmS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sentiment to number\n",
        "combine_data['label'], names = pd.factorize(combine_data['Sentiment'])# generating label encoder\n",
        "\n"
      ],
      "metadata": {
        "id": "rhjrev6rQy8u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = combine_data['FB_WSW']\n",
        "y = combine_data['label']\n",
        "\n",
        "# Perform stratified split: 80% for training, 20% for validation\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, stratify=y, random_state=3)\n",
        "\n",
        "\n",
        "# Combine the split data back into DataFrames for easier handling later\n",
        "train_dataset = pd.DataFrame({'FB_WSW': X_train, 'label': y_train})\n",
        "val_dataset = pd.DataFrame({'FB_WSW': X_val, 'label': y_val})\n"
      ],
      "metadata": {
        "id": "MhH9-3jQRFdo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset.to_csv('train_pandas.csv', index=False)\n"
      ],
      "metadata": {
        "id": "SXb5j7qLqyEd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset_ = tf.data.Dataset.from_tensor_slices((train_dataset['FB_WSW'].values, train_dataset['label'].values))\n",
        "val_dataset_ = tf.data.Dataset.from_tensor_slices((val_dataset['FB_WSW'].values, val_dataset['label'].values))\n",
        "\n",
        "train_ds = train_dataset_.shuffle(buffer_size=16).batch(16)\n",
        "val_ds = val_dataset_.shuffle(buffer_size=16).batch(16)\n"
      ],
      "metadata": {
        "id": "vgz5wl2ZQy5u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, j in train_dataset_.take(1):## remove\n",
        "  print(i.shape)"
      ],
      "metadata": {
        "id": "UVBBo5uokWWD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import layers\n",
        "\n",
        "max_length = 200\n",
        "max_tokens = 15000\n",
        "text_vectorization = layers.TextVectorization(\n",
        "    ngrams=2,\n",
        "    max_tokens=max_tokens,\n",
        "    output_mode=\"tf_idf\")\n",
        " # text_vectorization apply lower() and removing punctuations from the data\n",
        "\n",
        "text_only_train_ds = train_ds.map(lambda x, y: x)\n",
        "text_vectorization.adapt(text_only_train_ds)"
      ],
      "metadata": {
        "id": "PtXxxkFgQy3Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_int = train_ds.map( lambda x, y: (text_vectorization(x), y), num_parallel_calls=4)\n",
        "val_int = val_ds.map( lambda x, y: (text_vectorization(x), y), num_parallel_calls=4)\n"
      ],
      "metadata": {
        "id": "mCMlwMTMQy02"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "vocab_size = len(text_vectorization.get_vocabulary())\n",
        "vocab_size"
      ],
      "metadata": {
        "id": "ssIFzh_jRlcH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DL model"
      ],
      "metadata": {
        "id": "pTKD7brQ3Pe4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def get_model(max_tokens=vocab_size, hidden_dim=3):\n",
        "     inputs = keras.Input(shape=(vocab_size,))\n",
        "     x = layers.Dense(hidden_dim, activation=\"relu\")(inputs)\n",
        "     x = layers.Dropout(0.5)(x)\n",
        "     outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "     model = keras.Model(inputs, outputs)\n",
        "\n",
        "     model.compile(optimizer=\"rmsprop\",\n",
        "     loss=\"binary_crossentropy\",\n",
        "     metrics=[\"accuracy\"])\n",
        "     return model"
      ],
      "metadata": {
        "id": "hXbyC4k5QyyP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = get_model()\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "3PbuOzR0QynC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "callbacks = [\n",
        " keras.callbacks.ModelCheckpoint(\"/content/Curenetics.keras\",\n",
        " save_best_only=True)\n",
        "]\n",
        "\n",
        "model.fit(train_int,\n",
        " validation_data=val_int,\n",
        " epochs=40,\n",
        " callbacks = callbacks)\n"
      ],
      "metadata": {
        "id": "n5n-ms9STAB1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_model = tf.keras.models.load_model('/content/Curenetics.keras')\n"
      ],
      "metadata": {
        "id": "-sLEZ7yTijZ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts = ['it makes no sense at all ', 'i hate Movies with sad ending', 'page loads slow frustrates every time', 'The form submission failed multiple times', 'found website confusing hard use', ' the display is horrible', 'Great site great site love it love it', 'i love you']\n",
        "\n",
        "\n",
        "for text in texts:\n",
        "  text = text_vectorization([text])\n",
        "  predict = loaded_model.predict(text)\n",
        "  if predict < 0.5:\n",
        "      print('Positive')\n",
        "  else:\n",
        "      print('Negative')\n",
        "\n",
        "  print(predict)\n",
        "  #print(index_predict)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "eNOhzZsNrpbU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unbatched_test_ds = val_int.unbatch()\n",
        "\n",
        "# Batch all examples at once for analysis\n",
        "test_inputs, test_labels = [], []\n",
        "\n",
        "for inputs, labels in unbatched_test_ds:\n",
        "    test_inputs.append(inputs)\n",
        "    test_labels.append(labels)\n",
        "\n",
        "test_inputs = np.array(test_inputs)\n",
        "test_labels = np.array(test_labels)\n",
        "\n"
      ],
      "metadata": {
        "id": "G1Ouz7dtBHzP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "prediction = []\n",
        "predict = loaded_model.predict(test_inputs)\n",
        "for i in predict:\n",
        "  if i < 0.5:\n",
        "      prediction.append(0)\n",
        "  else:\n",
        "      prediction.append(1)\n",
        "\n",
        "\n",
        "print(\"Accuary: \", accuracy_score(test_labels, prediction))\n",
        "print(confusion_matrix(test_labels,prediction))\n",
        "\n"
      ],
      "metadata": {
        "id": "XRtYqA-oA0Wf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cm = confusion_matrix(test_labels, prediction)\n",
        "\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=names[np.unique(test_labels)])\n",
        "disp.plot(cmap='viridis')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "KYdXwj-xA0KS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Logistic Regression\n"
      ],
      "metadata": {
        "id": "WnPJtqTx2oL8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### tf dataset converted to np for logistic regression"
      ],
      "metadata": {
        "id": "Mdu1qGkp4FA3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def dataset_to_numpy(datasets):\n",
        "    X, y = [], []\n",
        "    for batch_x, batch_y in datasets:\n",
        "        X.append(batch_x.numpy())\n",
        "        y.append(batch_y.numpy())\n",
        "    return np.vstack(X), np.concatenate(y)\n",
        "\n",
        "X_train, y_train = dataset_to_numpy(train_int)\n",
        "X_val, y_val = dataset_to_numpy(val_int)\n",
        "\n"
      ],
      "metadata": {
        "id": "HfqhHlMF2ml3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "sl_model = LogisticRegression(max_iter=20)\n",
        "sl_model.fit(X_train, y_train)\n"
      ],
      "metadata": {
        "id": "7dP6aFVo2-3T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score\n",
        "\n",
        "\n",
        "prediction = sl_model.predict(X_train)"
      ],
      "metadata": {
        "id": "pXHI-gUY3D6W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Accuary: \", accuracy_score(y_train, prediction))\n",
        "print(confusion_matrix(y_train,prediction))\n",
        "\n"
      ],
      "metadata": {
        "id": "CLKw1Qry3JQI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prediction_val = sl_model.predict(X_val)\n",
        "\n",
        "print(\"Accuary: \", accuracy_score(y_val, prediction_val))\n",
        "print(confusion_matrix(y_val, prediction_val))\n",
        "\n"
      ],
      "metadata": {
        "id": "IjgGAu8Z3ZW0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cm = confusion_matrix(y_val, prediction_val)\n",
        "\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=names[np.unique(y_val)])\n",
        "disp.plot(cmap='viridis')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vjIwpYgCGzPB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import keras\n",
        "print(f\"TensorFlow version: {tf.__version__}\")\n",
        "print(f\"Keras version: {keras.__version__}\")"
      ],
      "metadata": {
        "id": "grh2gSWRFebv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary\n",
        "\n",
        "### Sentiment analysis trained with DL and logistic regress\n",
        "### Augemented with transformer and nltk\n",
        "### Logistic regression performed better likely due to very small dataset - DL Val_accuracy -80.3, logistic regression accuracy  - 93\n",
        "### The DL  shows steady training accuracy around 80-87% and consistent validation accuracy at 77-81%, with gradually decreasing loss. This indicates stable learning and good generalization, though validation accuracy plateaus, suggesting further dataset may be needed to boost performance.\n",
        "### Model used down stress for application"
      ],
      "metadata": {
        "id": "WB5WXvkA3jKX"
      }
    }
  ]
}